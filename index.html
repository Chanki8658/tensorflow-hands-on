<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Deeplearning</title>

		<meta name="description" content="Basic Deep Learning architectures and applications">
		<meta name="author" content="Ahmed TOUATI">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="reveal.js-3.0.0/css/reveal.css">
		<link rel="stylesheet" href="reveal.js-3.0.0/css/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="reveal.js-3.0.0/lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js-3.0.0/css/print/pdf.css' : 'reveal.js-3.0.0/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
		<style>
		html .reveal td  {
			vertical-align: middle !important;
		}
		.reveal section img {
			border :0
		}
		</style>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Deep Learning: An overview</h3>
					<p>Ahmed Touati</p>
					<p>Data Scientist @ AXA Data Innovation Lab</p>
					<img width="200" height="50" data-src="reveal.js-3.0.0/images/dil.png">
				</section>
				<section>
					<h3>Success stories</h3>
						<ul>
							<li>Record performance
								<ul><small>
									<li>MNIST (1988, 2003, 2012)</li>
									<li>ImageNet (since 2012) and Object Recognition</li>
									<li>...</li>
								</small></ul>
							</li>
							<li>Real applications
								<ul><small>
									<li>Check reading (AT&T Bell Labs, 1995 – 2005)</li>
									<li>Optical character recognition (Microsoft OCR, 2000)</li>
									<li>Cancer detection from medical images (NEC, 2010)</li>
									<li>Object recognition (Google and Baidu’s photo taggers, 2013)</li>
									<li>Speech recognition (Microsoft, Google, IBM switched in 2012)</li>
									<li>Natural Language Processing (NEC 2010)</li>
									<li>...</li>
								</small></ul></li>
						</ul>
				</section>
				<section>
					<p>Plan</p>
					<ol>
						<li>Why deep learning? history and motivation</li>
						<li>Learning algorithm and regularization</li>
						<li>Computer vision: convolutional neural network</li>
						<li>Natural language processing: recurrent neural network</li>

					</ol>	
				</section>
				<section>
					<h3>Historical waves of artificial neural networks</h3>
					
					<ul><small>
						<li>Cyberneticsin the 1940s-1960s with the developement of theories of biological learning and implementation of perceptron</li>
						<li>Connectionism approach with back-propagation to train neural network with one or two hidden layer</li>
						<li>Deep learning since 2006.</li> 
					</small></ul>
					<img height="300" data-src="reveal.js-3.0.0/images/google_books.png">
				</section>
				<section>
					<h3>Cybernetics (1948)</h3>
					<img height="300" data-src="reveal.js-3.0.0/images/norbert_wiener.jpg">
					<img height="300" data-src="reveal.js-3.0.0/images/cybernetics.png">
					<ul><small>
						<li>Cybernetics connects control (actions taken in hope of achieving goals) with communication (connection and information flow between the actor and the environment).</li>
						<li>Systems achieve goals through iterative processes or “feedback” loops</li>
					</small></ul>
				</section>
				<section>
					<h3>McCulloch & Pitts (1943)</h3>
					<img height="250" data-src="reveal.js-3.0.0/images/neuron.png">
					<ul>
						<li>A simplified neuron model: the Linear Threshold Unit.</li>
					</ul>
				</section>
				<section>
					<h3>Perceptron: first trained machine</h3>
					<ul>
						<li>First learning machine build at Cornell in 1960</li>
						<li>The perceptron was a linear classifier on top of a simple features extractor</li>
						<li> $y = \text{sign}(\sum_{i=1}^Nw_iF_i(x)+b)$ </li>
					</ul>
					<img width="300" height="300" data-src="reveal.js-3.0.0/images/perceptron.jpg">
					<img width="300" height="300" data-src="reveal.js-3.0.0/images/perceptron_diagram.png">
				</section>
				<section>
					<h3>Limitation of linear classifier</h3>
					<div class="half">
						<ul>
							<li><strong>Cover theorem 1966:</strong> Let $\{x_1, x_2...x_P\}$ be vector in $\mathbb{R}^N$, the number of distinct dichotomies that can be realized by a plane is: $C(N, P) = 2\sum_{k=0}^{N-1}(_{k}^{P-1})$</li>
							<!-- <li>A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a low-dimensional space.</li> -->
							<li>Abandon perceptrons and other analog computers.</li>
							<li>Develop symbolic computers and symbolic AI techniques.</li>
						</ul>
						
					</div>
					<div class="half">
						<img height="250" data-src="reveal.js-3.0.0/images/cover.png">
						<img height="300" data-src="reveal.js-3.0.0/images/perceptrons_book.jpg">
					</div>
				</section>
				<section>
					<h3>Symbolic AI</h3>
					<ul>
						<li>Developpement of knowledge-based system caoable of giving advice in paticular domain utilizing knowledge provided by human experts.</li>
						<li>Tow main components:
							<ul>
								<li>Knowledge represented in a number of ways such as rules, frames and cases.</li>
								<li>Inference engine which uses knowledge to arrive at conclusion.</li>
							</ul>
						</li>
					</ul>
				</section>
				<section>
					<h3>Quillian’s hierarchical propositional model (1968)</h3>
					<img height="500" data-src="reveal.js-3.0.0/images/Quillian.png">
				</section>
				<section>
					<h3>Connectionism</h3>
					<ul>
						<li>From psychological ideas of the XIXth and XXth centuries.</li>
						<li>The central connectionist principle is that mental phenomena can be described by interconnected networks of simple and often uniform units.</li>
						<li>Parallel Distributed Processing:
							<ul>
								<li>Neural representations are distributed.</li>
								<li>Neural computation is parallel.</li>
								<li>Processing units, connectivity, propagation rule, learning rule.</li>
							</ul>
						</li>
					</ul>
				</section>
				<section>
					<ul>Rumelhart’s propositional network</ul>
					<img height="600" width="700" data-src="reveal.js-3.0.0/images/Rumelhart.png">
				</section>
				<section>
					<h3>Training Network</h3>
					<ul>
						<li>Replace threshold unit by sigmoid unit.</li>
						<li>Collect training examples: 
							$$(..(\text{Item}_{i}, \text{Relation}_{i}, \text{DesiredOutput}_i)..)$$</li>
						<li>Form the mean squared error: 
							$$ E = \sum_k(\text{DesiredOutput}(k) - \text{Output}(k))^2$$</li>
						<li>Initialize with random weights and optimize by gradient descent.</li>
					</ul>
				</section>
				<section>
					<h3>Activations</h3>
					<img data-src="reveal.js-3.0.0/images/activation_epochs.png">
				</section>
				<section>
					<h3>Representations</h3>
					<img height="500" data-src="reveal.js-3.0.0/images/representation_epochs.png">
				</section>
				<section>
					<h3>Representations</h3>
					<div class="half">
						<img height="400" data-src="reveal.js-3.0.0/images/representation_cluster.png">
					</div>
					<div class="half">
						<img height="400" data-src="reveal.js-3.0.0/images/representation_pca.png">
					</div>
				</section>

				<section>
					<h3>Idea: Basis Function</h3>
					<div class="half">
						<ul>
							<li> $f(x, w) = \sum_{i=1}^mw_ih_i(x)$</li>
							<li>Basis function may be sample-centered (kernels) or centered in areas of high sample density (using an unsupervised clustering algorithm)</li>
							<li>An interesting type of basis function is $\sigma(U^{T}x) = \frac{1}{1+\exp(-U^{T}x)}$ (<span style="color:red">single hidden layer neural network</span>)</li>
						</ul>
					</div>
					<div class="half">
						<img height="300" width="400" data-src="reveal.js-3.0.0/images/rbf_kernel.png">
						<img data-src="reveal.js-3.0.0/images/RBF.png">
					</div>
				</section>
				<section>
					<ul>
						<li><strong>Universal approximation theorem (Cybenko 1989):</strong></li> Let $\phi$ be bounded and monotonically-increasing continuous function. The space of function the form
						$$ f(x) = \sum_{i=1}^Nv_i\phi_i(w_i^Tx + b_i), N \in \mathbb{I}, v_i, b_i \in \mathbb{R}, w_i \in \mathbb{R}^m$$
						are dense in the space of continuous function on $[0, 1]^m$
						<li class="fragment"><strong>Theoritician's dilemma:</strong> We can approximate any function as close as we want with shallow architecture. Why we need deep ones?</li>
					</ul>
				</section>
				<section>
					<h3>The need of depth</h3>
					<ul>
						<li><strong>N-bit parity</strong>: requires N-1 XOR gates in a tree of $\log(N)$ where it requires an exponential gates if we restrict ourselves to 2 layers.</li>
					</ul>
					<img data-src="reveal.js-3.0.0/images/N_parity_XOR.png">
					<img data-src="reveal.js-3.0.0/images/DNF.png">
				</section>
				<section>
					<h3>The need of depth</h3>
						<ul><small>
							<li>A series of hidden layers extracts increasingly abstract features from the image.</li>
							<li> the ﬁrst layer can easilyidentify edges, by comparing the brightness of neighboring pixels.</li>
							<li> the second hidden layer can easily search for corners and extended contours, which are recognizable as collections of edges.</li>
						</small></ul>
						<img height="400" data-src="reveal.js-3.0.0/images/hierarchical.png">
				</section>
				<section>
				<img>What does deep learning became very popular now?</img>
					<small><ul>
						<li>Deep learning has become more useful as the amount of available training data has increased.</li>
						<li>Deep learning models have grown in size over time as computer hardware and software infrastructure for deep learning has improved (nvidia GPU, Google TPU).</li>
					</ul></small>
					<img height="500" data-src="reveal.js-3.0.0/images/imagenet.png">
				</section>
				<section>
					<h3>Dataset Over Algorithm</h3>
					<img data-src="reveal.js-3.0.0/images/dataset_over_algorithm.png">
					<small>The average elapsed time between key algorithm proposals and corresponding advances was about 18 years, whereas the average elapsed time between key dataset availabilities and corresponding advances was less than 3 years, or about 6 times faster.</small>
				</section>
				<section>
					<h3>Imagenet Large Scale Visual Recognition Challenge</h3>
					<img data-src="reveal.js-3.0.0/images/imagenet_results.jpg">
					<small>
						Error rates on the imagenet dataset have fallen dramatically since the introduction of deep learning systems for classification and localization of objects in images.
						<a href="http://image-net.org/challenges/LSVRC/2011/results">Imagenet 2011 results</a>,
						<a href="http://image-net.org/challenges/LSVRC/2012/results">Imagenet 2012 results</a>,
						<a href="http://image-net.org/challenges/LSVRC/2013/results">Imagenet 2013 results</a>,
						<a href="http://image-net.org/challenges/LSVRC/2014/results">Imagenet 2014 results</a>,
						<a href="http://image-net.org/challenges/LSVRC/2015/results">Imagenet 2015 results</a>.
					</small>
				</section>
				<section>
					<h3>Imagenet Large Scale Visual Recognition Challenge</h3>
					<img data-src="reveal.js-3.0.0/images/imagenet_models.png">

				</section>
	
				<section>
					<h3>Multilayer perceptron</h3>
					<div class="half">
						<p><strong>Neural network with L hidden layers</strong></p>
						<small><ul>
							<li>layer pre-activation for $k>0$ ($h^{(0)}(x)=x$): 
								$$ a^{(k)}(x) = b^{(k)} + W^{(k)}h^{(k-1)}(x)$$</li>
							<li>hidden layer activation ($k \in \{1,..L\}$):
							$$ h^{(k)}(x) = g(a^{(k)}(x))$$ </li>
							<li>output layer activation (k=L+1):
							$$ h^{(L+1)}(x) = o(a^{(L+1)}(x)) = f(x)$$</li>
						</ul></small>
					</div>
					<div class="half">
						<img width="400" data-src="reveal.js-3.0.0/images/mlp.png">
					</div>
				</section>
				<section>
					<h3>Activation function</h3>
					<div class="half">
						<small><ul>
							<li>sigmoid function: $$g(x) = \frac{1}{1+\exp(-x)}$$</li>
							<li>Tanh function: $$g(x) = \tanh(x) = \frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)}$$</li>
							<li>ReLu (rectified linear unit)function: $$g(x) = max(x, 0)$$</li>
							<li>We use the softmax activation function at the output layer:
					$$o(a) = \text{softmax}(a) = [\frac{\exp(a_1)}{\sum_c \exp(a_c)},..\frac{\exp(a_C)}{\sum_c \exp(a_c)} ]$$</li>
						</ul></small>
					</div>
					<div class="half">
						<img height="150" data-src="reveal.js-3.0.0/images/sigmoid.png">

						<img height="150" data-src="reveal.js-3.0.0/images/tanh.png">
						<img height="150" data-src="reveal.js-3.0.0/images/reLu.png">
					</div>
				</section>
				<section>
					<h3>Gradient descent algorithm</h3>
						<small>
						<ul>
							<li>initialize $\theta, (\theta = \{W^{(1)}, b^{(1)},.. W^{(L+1)}, b^{(L+1)}\} )$</li>
							<li>for K iterations:
									$$ \delta = - \frac{1}{N}\sum_{t \in \{1,..N\}}\nabla_{\theta}l(f(x^{(t)}; \theta), y^{(t)})$$
									$$ \theta_{k+1} = \theta_k + \eta \delta $$
							</li>
							<li>For classification, the loss function is the negative log-likelihood $f(x)_c = p(y=c|x)$ (sometimes referred to as cross-entropy):
								$$l(f(x), y) = - \sum_c 1_{(y=c)}\log f(x)_c = - \log f(x)_y $$</li>
						</ul>
					</small>
				</section>
				<section>
					<h3>Backpropagation algorithm</h3>
					<div class="half">
						<img width="400" data-src="reveal.js-3.0.0/images/Rumelhart_paper_title.png">
						Rumelhart and al 1986: First time back-propagation became popular
					</div>
						<img width="400" data-src="reveal.js-3.0.0/images/Rumelhart_back_prop.png">
				</section>
				<section>
					<h3>Gradient computation</h3>
					<div class="half">
						<p><strong>Loss gradient at output pre-activation:</strong></p>
						<small>
						<ul>
							<li>Partial derivative:
								$$\frac{\partial}{\partial a^{(L+1)}(x)_c}- \log f(x)_y$$
								$$= - (1_{(y=c)} - f(x)_c)$$</li>
							<li>Gradient:
								$$\nabla_{a^{(L+1)}(x)} (-\log f(x)_y)$$
								$$= - (e(y) - f(x))$$</li>
						</ul>
					</small>
					</div>
					<div class="half">
						<img data-src="reveal.js-3.0.0/images/pre_activation_gradient.png">
					</div>
				</section>
				<section>
					<h3>Gradient computation</h3>
					<div class="half">
						<strong>Chain rule</strong>
						<small>
						<ul>
							<li>if a function p(a) can be written as a function of intermediate results $q_i(a)$ then we have: 
								$$ \frac{\partial p(a)}{\partial a} = \sum_i \frac{\partial p(a)}{\partial q_i(a)} \frac{\partial q_i(a)}{\partial a} $$</li>
						</ul>
					</small>
					<img data-src="reveal.js-3.0.0/images/chain_rule.png">
					</div>
					<div class="half">
						<img data-src="reveal.js-3.0.0/images/hidden_layer_gradient.png">
					</div>
				</section>
				<section>
					<h3>Gradient computation</h3>
					<strong>Loss gradient at hidden layers</strong>: let's apply chain rule
					<small><ul>
						<li>Partial derivative: 
							$$ \frac{\partial}{\partial h^{(k)}(x)_j} -\log f(x)_y = 
							\sum_i \frac{\partial - \log f(x)_y}{\partial a^{(k+1)}(x)_i} \frac{\partial a^{(k+1)}(x)_i}{\partial h^{(k)}(x)_j}$$</li>
						<li>Gradient: 
							$$\nabla_{h^{(k)}(x)} - \log f(x)_y = (W^{(k+1)})^T	(\nabla_{a^{(k+1)}(x)} - \log f(x)_y)$$</li>
					</ul></small>
				</section>
				<section>
					<h3>Gradient computation</h3>
					<div class="half">
						<p><strong>Loss gradient at hidden layers pre-activation</strong></p>
						<!-- <ul> -->
						<small>
							<ul>
							<li>Partial derivative:
							\begin{align}
							&\frac{\partial}{\partial a^{(k)}(x)_j} - \log f(x)_y\\
							 &= \frac{\partial - \log f(x)_y}{\partial h^{(k)}(x)_j}  
							\frac{\partial h^{(k)}(x)_j}{\partial a^{(k)}(x)_j}\\
							 &= \frac{\partial - \log f(x)_y}{\partial h^{(k)}(x)_j} g'(a^{(k)}(x)_j)
							 \end{align}</li>
							<li>Gradient:</li>
							$$\nabla_{a^{(k)}} - \log f(x)_y = (\nabla_{h^{(k)}(x)}-\log f(x)_y) . [..g'(a^{(k)}(x)_j..]$$
						</ul></small>
						<!-- </ul> -->
					</div>
					<div class="half">
						<img width="350" data-src="reveal.js-3.0.0/images/hidden_pre_activation.png">
					</div>
				</section>
				<section>
					<h3>Gradient computation</h3>
					<p><strong>Loss gradient at parameters</strong></p>
					<small><ul>
						<li>Partial derivative:
						$$\frac{\partial}{\partial W^{(k)}_{i,j}} - \log f(x)_y
						 = \frac{\partial - \log f(x)_y}{\partial a^{(k)}(x)_i}  
						\frac{\partial a^{(k)}(x)_i}{\partial W^{(k)}_{i,j}}$$
						$$\frac{\partial}{\partial b^{(k)}_{i}} - \log f(x)_y 
						= \frac{\partial - \log f(x)_y}{\partial a^{(k)}(x)_i}  
						\frac{\partial a^{(k)}(x)_i}{\partial b^{(k)}_i}$$</li>
						<li>Gradient:
							$$\nabla_{W^{(k)}}-\log f(x)_y = (\nabla_{a^{(k)}}-\log f(x)_y)h_{(k-1)}(x)^T$$
							$$\nabla_{b^{(k)}}-\log f(x)_y = \nabla_{a^{(k)}}-\log f(x)_y$$</li>

					</ul></small>
				</section>
				<section>
					<h3>Backpropagation algorithm</h3>
					<small><ul>
						<li >compute output gradient</li>
						$$\nabla_{a^{(L+1)}(x)_c} (-\log f(x)_y) = - (e(y) - f(x))$$
						<li class="fragment">for k from L+1 to 1:
							<ul>
								<li>compute gradient of hidden layer parameter:
								$$\nabla_{W^{(k)}}-\log f(x)_y = (\nabla_{a^{(k)}}-\log f(x)_y)h_{(k-1)}(x)^T$$
							$$\nabla_{b^{(k)}}-\log f(x)_y = \nabla_{a^{(k)}}-\log f(x)_y$$</li>
								<li class="fragment">compute gradient of hidden layer below:
									$$\nabla_{h^{(k-1)}(x)} - \log f(x)_y = (W^{(k)})^T \nabla_{a^{(k)}(x)} - \log f(x)_y$$
									\begin{align}
									\nabla_{a^{(k-1)}} - \log f(x)_y &= (\nabla_{h^{(k-1)}(x)}-\log f(x)_y) \\
									&. [..g'(a^{(k-1)}(x)_j..]
									\end{align}
								</li>
							</ul>
						</li>
					</ul></small>
				</section>
				<!-- <section>
					<h3>Gradient descent</h3>
					<small><ul>
						<li>Batch gradient descent: we look at all the examples at the same time. It is not efficient when we have a massive dataset</li>
						<li>Stochastic gradient descent: use one random example. It avoids the whole data summations</li>
						<li>Mini-batch gradient descent: could be even more faster, it uses m examples in each iteration</li>
					</ul>
				</small>
					<img height="300" data-src="reveal.js-3.0.0/images/SGD.png">
					<img height="300" data-src="reveal.js-3.0.0/images/GD.png">
				</section> -->
				<section>
					<h3>Gradient descent</h3>
					<div class="half">
						<small>
							<strong>Batch Optimization</strong>
							$$ \theta_{k+1} = \theta_k - \eta \frac{1}{N}\sum_{t \in \{1,..N\}}\nabla_{\theta}l(f(x^{(t)}; \theta), y^{(t)})$$
							<ul>
								<li>More expensive step</li>
								<li>Can choose among a wide range of optimization algorithms</li>
								<li>Opportunities for parallelism</li>
							</ul>
						</small>
						<img height="300" data-src="reveal.js-3.0.0/images/GD.png">
					</div>
					<div class="half">
						<small>
							<strong>Stochastic Optimization</strong>
							$$ \theta_{k+1} = \theta_k - \eta \nabla_{\theta}l(f(x^{(t)}; \theta), y^{(t)})$$
							$$ t \in \{1,...,N\} \text{is chosen randomly}$$
							<ul>
								<li>Very cheap iteration; gradient w.r.t. just 1 data point</li>
								<li>Stochastic process dependent on the choice of i</li>
								<li>Descent in expectation</li>
							</ul>
						</small>
						<img height="300" data-src="reveal.js-3.0.0/images/SGD.png">
					</div>
				</section>
				<section>
					<h3>Theoretical motivation: strongly convex case</h3>
					<div class="half">
					<small>
						<ul>
						<li>Let's $R_n(\theta) = \frac{1}{n}\sum_{t \in \{1,...,n\}}l(f(x^{(t)}; \theta), y^{(t)})$ and $\theta^*=\text{argmin}(R_n)$</li>
						
							<li><strong>Batch gradient: linear convergence</strong>
								$$R_n(\theta_k)-R_n(\theta^*) \leq O(\rho^k)$$
							Per iteration cost proportional to n</li>
							<li><strong>Stochastic gradient: sublinear convergence</strong>
								$$\mathbb{E}[R_n(\theta_k)-R_n(\theta^*)] = O(\frac{1}{k})$$
							Per iteration cost and convergence constant independant of n</li>
						</ul>
						<table>
						<thead>
							<tr>
								<th></th>
								<th>Batch</th>
								<th>Stochastic</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>convergence rate</td>
								<td>$\rho^k$</td>
								<td>$\frac{1}{k}$</td>
							</tr>
							<tr>
								<td>$\epsilon$ optimality</td>
								<td>$n\log(\frac{1}{\epsilon})$</td>
								<td>$\frac{1}{\epsilon}$</td>
							</tr>
						</tbody>
					</table>
					</small>
				</div>
				<div class="half">
					Practical motivation
					<img height="300" data-src="reveal.js-3.0.0/images/sgd_epochs.png">
					<p><small>Fast initial progress of SG followed by drastic slowdown</small></p>
				</div>
				</section>
				<section>
					<h3>Beyond stochastic GD</h3>
					<small><strong>Mini-batch gradient descent:</strong> is widely used in practice:
					$$\theta_{k+1} = \theta_k - \eta \frac{1}{|S_k|}\sum_{t \in S_k}\nabla_{\theta}l(f(x^{(t)}; \theta), y^{(t)})$$
					$$ S_k \subset \{1,...,n\} \text{is chosen randomly in each iteration}$$
					<ul>
						<li>allows some degree of parallelization</li>
						<li>reduces variance of the stochastic gradients estimates</li>
					</ul>

					</small>
					<img height="300" data-src="reveal.js-3.0.0/images/opt_spectrum.png">
					<p><small>Schematic of two-dimensional spectrum of optimization methods</small></p>
					
				</section>
				<section>
					<h3>Second-order methods</h3>
					<div class="half">
					<small>
						<p>Batch gradient step $\eta \nabla_{\theta}F(\theta_k)$ ignores curvature of the function:</p>
						<img height="200" data-src="reveal.js-3.0.0/images/gd_update.png">
					</small>
					</div>

					<div class="half">
					<small>
						<p>Newton scaling $B = (\nabla_{\theta}^2F(\theta_k))^{−1/2}$: gradient step moves to the minimizer:</p>
						<img height="200" data-src="reveal.js-3.0.0/images/newton_scaling.png">
					</small>
					</div>
					<div class="fragment">
					<small>
					<p>..corresponds to minimizing a quadratic model of F in the original space
					$\theta_{k+1} = \theta_k - \eta s_k, \text{ where } \nabla_{\theta}^2F(\theta_k)s_k = -\nabla_{\theta}F(\theta_k)$
				</p></small>
					<img height="200" data-src="reveal.js-3.0.0/images/second_order.png">
				</div>
				</section>
				<section>
					<h3>Gradient descent methods</h3>
					<small><ul>
						<li>We recall the gradient descent update: $$x_{t+1} = x_t + \delta_t, \text{where } \delta_t = -\eta g_t$$</li>
						<li><strong>Adagrad</strong>: large gradients have smaller learning rates and small gradients have large learning rates:
							$$ \delta_t = \frac{\eta}{\sqrt{\sum_{\tau=1}^t g_{\tau}^2}}g_t$$</li>
						<li><strong>Momentum</strong>: accelerates progress along dimensions in which gradient consistently point in the same direction and to slow progress along dimensions where the sign of the gradient continues to change.
							$$ \delta_t = \rho \delta_{t-1} - \eta g_t$$</li>
						
					</ul></small>
					<img height="300" data-src="reveal.js-3.0.0/images/momentum_update.png"> 
				</section>
				<section>
					<h3>Gradient descent methods</h3>
					<small><ul>
						<li><strong>Nesterov Method</strong>: First make a big jump in the direction of the previous accumulated gradient. Then measure the gradient where you end up and make a correction.</li>
						<img height="300" data-src="reveal.js-3.0.0/images/nesterov_update.png">

						<li>Other approches: <strong>Adam</strong>, <strong>AdaDelta</strong> and <strong>RMSProp</strong></li>
					</ul></small>
				</section>


				<section>
					<h3>What have we seen last time?</h3>
					<ul>
						<li>Multilayer perceptron: basic neural architecture</li>
						<li>Learning algorithm: gradient descent</li>
						<li>Backpropagation: efficient way to compute gradient</li>
						<li>Gradient based optimization: Theoretical and practical motivation</li>
					</ul>
				</section>
				<section>
					<h3>What will we see today?</h3>
					<ul>
						<li>Non convex optimization</li>
						<li>Regularization</li>
						<li>Convolutional neural network: this is the main part</li>
						<li>What does a neural network like in tensorflow?</li>
					</ul>
				</section>
				<section>
					<h3>Non convex Optimzation</h3>
				</section>
				<section>
					<h3>The mirage of convexity</h3>
					<small><ul>
						<li>Local minima dominate in low-D, but saddle points dominate in high-D</li>
						<li>Most local minima are close to the bottom (global minimum error)</li>
					</ul></small>
					<img height="300" data-src="reveal.js-3.0.0/images/convexity.png">
					<img height="300" data-src="reveal.js-3.0.0/images/saddle_point.png">
				</section>
				<section>
					<h3>Are saddle points or local minima more common?</h3>
					<ul>
						<li class="fragment">Imagine for each eigenvalue, you flip a coin</li>
						<li class="fragment">If heads, the eigenvalue is positive, if tails, negative</li>
						<li class="fragment">Need to get all heads to have a minimum</li>
						<li class="fragment">Higher dimensions -> exponentially less likely to get all heads</li>
						<li class="fragment">Random matrix theory: The coin is weighted; the lower R is, the more likely to be heads</li>
						<li class="fragment">So most local minima have low R!</li>
						<li class="fragment">Most critical points with high J are saddle points!</li>
					</ul>
					<small class="fragment">[Ian Goodfellow, Deep Learning Summer School Montreal]</small>
				</section>
				<section>
					<h3>Saddle points during traning</h3>
					<small><ul>
					Oscillaing between two behaviors:
					<li>Slowly approaching a saddle point.</li>
					<li>Escaping it</li>
					</ul></small>
					<img height="400" data-src="reveal.js-3.0.0/images/saddle_training.png">
				</section>
				<section>
					<h3>Regularization</h3>
				</section>
				<section>
					<h3>Regularization: Dropout</h3>
					<small>A Simple Way to Prevent Neural Networks from Overfitting</small>
					<img height="300" data-src="reveal.js-3.0.0/images/dropout.png">
					<img height="200" data-src="reveal.js-3.0.0/images/dropout_train.png">
				</section>

				<section>
					<h3>Regularization: Dropout</h3>
					<small><ul>
						<li><strong>Dropout is form of model averaging</strong>: let's consider a single hidden layer network with H hidden units. Dropout is equivalent to sampling from $2^H$ models. Each models only gets one traning example.</li>
						<li>At evaluation time, instead of sampling many different models and take geometric mean of their output distribution, we use one model with all the hidden units activated but we halve their outgoing weights.</li>
					</ul>
				</small>
				<div class="fragment">
				<img height="300" data-src="reveal.js-3.0.0/images/hinton_dropout_slide.png">
				<small>Geoffrey hinton's online course of neural networks for machine learning</small>
			</div>
				</section>
				
				<section>
					<h3>Computer vision</h3>
					<div class="half">
						Classification task example:
						
						<img height="270" data-src="reveal.js-3.0.0/images/chat.png">
						<img height="270" data-src="reveal.js-3.0.0/images/localization.png">
					</div>
					<div class="half">
						challenges:
						<ul>
							<li>View point variation</li>
							<li>Scale variation</li>
							<li>Illumination conditions</li>
							<li>Deformation</li>
							<li>...</li>
						</ul>
						<img data-src="reveal.js-3.0.0/images/def_img.png">
					</div>
				</section>
				<section>
					<h3>Speed of processing in the human visual system</h3>
					<img height="500" data-src="reveal.js-3.0.0/images/Thorpe.png">
				</section>
				<section>
					<h3>Hubel & Wiesel (1962)</h3>
					<ul>
						Biological insights:
						<li>Simple cells detect local features</li>
						<li>Complex cells pool local features in a retinotopic neighborhood</li>
					</ul>
					<img height="300" data-src="reveal.js-3.0.0/images/Hubel.png">
					<img height="300" data-src="reveal.js-3.0.0/images/hubel_experiment.jpg">
				</section>
				<section>
					<h3>The Neocognitron</h3>
					<img height="500" data-src="reveal.js-3.0.0/images/neocognitron.png">
				</section>
				<section>
					<h3>LeNet5 (1998)</h3>
					<img data-src="reveal.js-3.0.0/images/lenet_paper.png">
					<small>
					<ul>
						<li>Convolutional neural network use sequence of 3 layers: convolution, pooling, non-linearity.</li>
						<li>Use convolution to extract spatial features.</li>
						<li>Subsample using spatial average of maps.</li>
						<li>Non-linearity in the form of tanh or sigmoids.</li>
						<li>Multi-layer neural network (MLP) as final classifier.</li>
						<li>Sparse connection matrix between layers to avoid large computational cost.</li>
					</ul>
				</small>
				<img height="200" data-src="reveal.js-3.0.0/images/lenet.png">
			</section>
				<section>
					<h3>Local connexions</h3>
					<img height="500" data-src="reveal.js-3.0.0/images/local_connexion.png">
				</section>
				<section>
					<h3>Convolution</h3>
					<img height="450" data-src="reveal.js-3.0.0/images/convolution.png">
					<p><strong>Convolve</strong> the filter with images: slide over the images spatially and compute the dot products</p>  
				</section>
				<section>
					<h3>Multiple convolution</h3>
					<img height="500" data-src="reveal.js-3.0.0/images/multiple_convolution.png">
				</section>
				<section>
					<h3>Convolution</h3>
					<img height="600" data-src="reveal.js-3.0.0/images/conv1.png">
				</section>
				<section>
					<h3>Convolution</h3>
					<img height="600" data-src="reveal.js-3.0.0/images/conv2.png">
				</section>
				<section>
					<h3>Pooling layer</h3>
					<div class="half">
					<small><ul>
						<li>introduces local translation invariance</li>
						<li>reduces dimentionnality</li>
					</ul></small>
					<img data-src="reveal.js-3.0.0/images/max_pooling.png">
				</div>
				<div class="half">
					<img height="500" data-src="reveal.js-3.0.0/images/pooling_layer.png">	
				</div>

				</section>
				<section>
					<h3>Convolutional neural network (Convnet) layers</h3>
					<small>
						<ul>
							<li><strong>Input layer</strong>: holds the raw pixel values of the image. 3 dimensions: width, height, and depth (color channels).</li>
							<li class="fragment"><strong>Convolutional layer</strong>: computes the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and the region they are connected to in the input volume.</li>
							<li class="fragment"><strong>ReLu layer</strong>: applies an elementwise activation function.</li>
							<li class="fragment"><strong>Pooling layer</strong>: performs a downsampling operation along the spatial dimensions (width, height).</li>
							<li class="fragment"><strong>Fully connected layer</strong>: computes the class scores</li>
						</ul>
					</small>
					<img height="350" data-src="reveal.js-3.0.0/images/convnet_layers.png">
				</section>
				<section>
					<h3>Traditional approaches</h3>
					<small>Manually designed features are often over-specified, incomplete and take a long time to design and validate.</small>
					<img data-src="reveal.js-3.0.0/images/traditional_vision.png">
					<small>Example of descriptor: Scale-Invariant Feature Transform and vector quantization</small>
					<img data-src="reveal.js-3.0.0/images/sift.png">
				</section>
				<section>
					<h3>End to end training</h3>
					<img data-src="reveal.js-3.0.0/images/end_to_end.png">
					<img data-src="reveal.js-3.0.0/images/convnet.jpeg">
				</section>
				
				<section>
					<h3>ImageNet challenge</h3>
					14 million images gathered from internet and labeled via Amazon	Turk, 20k classes.
					<img data-src="reveal.js-3.0.0/images/imagenet.jpg">
				</section>
				<section>
					<h3>ImageNet challenge</h3>
					<ul>
						<li>Winner: SuperVision team: Alex Krizhevsky,Ilya Sutskever,Geoffrey Hinton, University of Toronto.</li>
						<li>Next best model is not convnet</li>
					</ul>
					<img height="400" data-src="reveal.js-3.0.0/images/imagenet_result.png">
				</section>
				<section>
					<h3>AlexNet architecture</h3>
					<ul>
						<li>7 layers, 650 000 neurons, 60 000 000 parameters</li>
						<li>Trained on 2 GPUs in two weeks</li>
						<li>Dropout with 0.5 probability</li>
						<li>In order to avoid overfitting, they trained on 224x224 patches extracted randomly from 256x256 images, and also their horizontal reflections</li>
					</ul>
					<img data-src="reveal.js-3.0.0/images/alexnet.png">
				</section>
				<section>
					<h3>Convnet visualizing</h3>
					<div class="half">
						<ul >
							<li>Use deconvolution to map activations at higher layers back to in the input</li>
							<li>same operations as Convnet but in reverse:
								<ul>
									<li>Unpool features maps</li>
									<li>Rectify unpooled maps with the same non-linearity</li>
									<li>Convolve rectified unpooled maps with transposed learned filters</li>
								</ul>
							</li>
						</ul>
					</div>
					<div class="half">
						<img class="half" width="700" data-src="reveal.js-3.0.0/images/deconvnet.png">
					</div>
				</section>
				<section>
					<h3>Convnet visualizing</h3>
					<ul>
						<li>For a given feature map, we show the top 9 activations. Projecting each separately down to pixel space reveals the different structures that excite a given feature map.</li>
					 	<li>Alongside these visualizations we show the corresponding image patches.</li>
					</ul>
					<img data-src="reveal.js-3.0.0/images/layer_1_2.png">
				</section>
				<section>
					<h3>Convnet visualizing</h3>
					<img height="620" data-src="reveal.js-3.0.0/images/layer_3_5.png">
				</section>

				<section>
				<h3>What does a neural network like in tensorflow?</h3>
				<div class="half">
					<img height="300" data-src="reveal.js-3.0.0/images/tensorflow.png">
					<small>TensorFlow is Google open-sourced machine learning library</small>
				</div>
				<div class="half">
					<img height="300" data-src="reveal.js-3.0.0/images/mnist.jpeg">
					<small>Hello word: handwritten digits classification</small>
				</div>
			</section>
			<section>
				<h3>Simple model: Softmax classification</h3>
				<img height="400" data-src="reveal.js-3.0.0/images/mnist_softmax.png">
				$$ P(y=k) = \frac{\exp(\sum_{i=1}^{784}w_{k,i}x_i + b_k)}{\sum_{k=0}^9\exp(\sum_{i=1}^{784}w_{k,i}x_i + b_k)} = \text{softmax}(\sum_{i=1}^{784}w_{k,i}x_i + b_k)$$
			</section>
			<section>
				<h3>matrix mutiplication: a batch of 100 images at a time</h3>
				<img height="300" data-src="reveal.js-3.0.0/images/mnist_logit.png">
				<small><p>X : Images [100, 784], W: Weights [784, 10], b : Biases [10], Y : predictions [100, 10]</p></small>
				<pre><code data-trim contenteditable>
					Y = tf.nn.softmax(tf.matmul(X, W) + b)
				</code></pre>
			</section>
			<section>
				<h3>Loss function</h3>
				<img height="300" data-src="reveal.js-3.0.0/images/mnist_cross_entropy.png">
				<pre><code data-trim contenteditable>
					cross_entropy = -tf.reduce_sum(Y_*tf.log(Y))
				</code></pre>
			</section>
			<section>
				<h3>Tensorflow: Initialisation</h3>
				<pre style="font-size: 28px"><code data-trim contenteditable>
import tensorflow as tf

X = tf.placeolder(tf.float32, [None, 28, 28, 1]) 
# None will become the batch size, 100
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

init = tf.initialize_all_variables()
				</code></pre>
			</section>
			<section>
				<h3>tensorflow: loss function and accuracy</h3>
				<pre style="font-size: 23px"><code data-trim contenteditable>
# model
Y = tf.nn.softmax(tf.matmul(tf.reshape(X, [-1, 784]), W) + b)
# placefolder for correct labels
Y_ = tf.placeholder(tf.float32, [None, 10])

# loss function
cross_entropy = -tf.reduce_sum(Y_*tf.log(Y))

# % of correct label in batch
is_correct = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
				</code></pre>	
			</section>
			<section>
				<h3>tensorflow: training</h3>
				<pre style="font-size: 27px"><code data-trim contenteditable>
optimizer = tf.train.GradientDescentOptimizer(0.001)
train_step = optimizer.minimize(cross_entropy)
				</pre></code>
			</section>
			<section>
				<h3>TensorFlow: RUN</h3>
				<pre><code data-trim contenteditable>
sess = tf.Session()
sess.run(init)
for i in range(1000):
# load batch images and labels
batch_X, batch_Y = mnist.train.next_batch(100)
  train_data={X: batch_X, Y_: batch_Y}

  # train
  sess.run(train_step, feed_dict=train_data)

  #success
  a, c = sess.run([accuracy, cross_entropy], feed_dict=train_data)

  # sucess on test data
  test_data={X: mnist.test.images, Y_: mnist.test.labels}
  a, c = sess.run([accuracy, cross_entropy], feed_dict=test_data)

				</code></pre>
			</section>
			<section>
				<h3>TensorFlow: All code</h3>
				<img height="600" data-src="reveal.js-3.0.0/images/all_code.png">
			</section>
			<section>
				<h3>Go deeper</h3>
				<img data-src="reveal.js-3.0.0/images/go_deeper.png">
			</section>
			<section>
				<h3>Tensorflow: Initialisation</h3>
				<pre><code data-trim contenteditable>
K = 200
L = 100
M = 60
N = 30
W1 = tf.Variable(tf.truncated_normal([28*28, K]))
b1 = tf.Variable(tf.zeros([K]))
W2 = tf.Variable(tf.truncated_normal([K, L]))
b2 = tf.Variable(tf.zeros([L]))
W3 = tf.Variable(tf.truncated_normal([L, M]))
b3 = tf.Variable(tf.zeros([M]))
W4 = tf.Variable(tf.truncated_normal([M, N]))
b4 = tf.Variable(tf.zeros([N]))
W1 = tf.Variable(tf.truncated_normal([N, 10]))
b1 = tf.Variable(tf.zeros([10]))
				</code></pre>
			</section>
			<section>
				<h3>TensorFlow: the model</h3>
				<pre><code data-trim contenteditable>
X = tf.reshape(X, [-1, 28*28])

Y1 = tf.nn.sigmoid(tf.matmul(X, W1) + b1)
Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + b2)
Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + b3)
Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + b4)

Y = tf.nn.softmax(tf.matmul(Y4, W5) + b5)
				</code></pre>
			</section>
			<section>
				<h3>Deep learning in natural language processing</h3>
				<img height="500" data-src="reveal.js-3.0.0/images/word_embeddings_colah.png">
			</section>
				<section>
					<h3>Deep learning in natural language processing</h3>
					<ul>
						<li>Goal: for computers to process or "understand" natural language in order to perform tasks that are useful.</li>
						<li>Applications range from simple to complex:
							<ul>
								<li>Spell checking, keyword search, finding synonyms</li>
								<li>Extracting information from websites such as product price, dates,..</li>
								<li>Classifying, reading level of school texts, positive/negative sentiment.</li>
								<li>Machine translation</li>
								<li>Question Answering</li>
							</ul>
						</li>
					</ul>
				</section>
				<section>
					<h3>What we will see today</h3>
					<ul>
						<li>Word embedding: word2vec</li>
						<li>Language model: Recurrent neural network and LSTM</li>
						<li>Basic NLP tasks: part of speech and named entity</li>
						<li>Sequence to sequence Framework: machine transation</li>
						<li>Attention mechanism: machine translation and image captioning</li>
					</ul>
				</section>
				<section>
					<h3>Word Embedding</h3>
				</section>
				<section>
					<h3>Vector Representations of Words</h3>
					<ul>
						<li>Image and audio processing system work with high-dimensional datasets encoded as vectors of the individual pixel-intensities or power spectral density coefficients</li>
						<li>Natural language processing systems traditionally treat words as discrete atomic symbols
						</li>

					</ul>
					<img height="200" data-src="reveal.js-3.0.0/images/why_word2vec.png">
				</section>
				<section>
					<h3>Representing Words - Classical View: One Hot Encoding</h3>
					<ul>
						<li>Obama: 10000</li>
						<li>Dolphin: 01000</li>
						<li>Paris: 00100</li>
						<li>Berlin: 00010</li>
						<li>SeaWorld: 00001</li>
					</ul>
					<img height="200" data-src="reveal.js-3.0.0/images/word_one_hot_encoding.png">
					<p>&#8658 Every word is orthogonal to one another $$(w_{Paris}. w_{Berlin} = 0)$$</p>
				</section>
				</section>
				<section>
					<h3>Word embedding</h3>
					<ul>
						<li>The Distributional Hypothesis in linguistics: words that are used and occur in the same contexts tend to purport similar meanings</li>
						<li>Word embedding represent words in a contiuous vector space where semantically similar words are mapped to nearby points.</li>
					</ul>
					<img height="300" data-src="reveal.js-3.0.0/images/word_space.png">
				</section>
				<section>
					<h3>Word embedding</h3>
					<p>Count based methods</p>
					<ul>
						<li>We compute the statistics of how some words co-occurs with its neighbor words in a large text corpus.</li>
						<li>Then we map these count-statistics down to a small, dense vector for each word.</li>
					</ul>
					<img data-src="reveal.js-3.0.0/images/LSA.png">
				</section>
				<section>
					<h3>Word embedding</h3>
					<p>Predictive Method: Word2vec (Mikolov, 2013)</p>
					<small>
						<ul>
							<li><strong>Continuous Bag Of Word (CBOW)</strong> architecture predicts the current word based on the context.</li>
							<li><strong>Skip-gram</strong> predicts surrounding words given the current word.</li>
							<li>Skip-gram treats each context-target pair as a new observation while CBOW smoothes over a lot of the distributional information</li>
						</ul>
					</small>
					<img height="400" data-src="reveal.js-3.0.0/images/cbow.png">

				</section>
				<section>
					<h3>Skip-gram</h3>
					
					<div class="half">
						<small>
						<ul>
							<li>Let's $W$ be embedding matrix and $W'$ the output weight matrix</li>
							<li>Let w a word is hot-one encoded by x such as $x_k=1, x_k=0 \text{ for } k' \neq k$, then, the hidden layer $$h = x^TW = W_{(k,.)} = v_w$$</li>
							<li>If $v'_{w_j}$ is the j-th column of the matrix $W'$, the output pre-activation is:
								$$ u_j= (v'_{w_j})^T h$$</li>
							
						</ul>
						</small>
					</div>

					<div class="half">
						<img height="500" data-src="reveal.js-3.0.0/images/skip_gram.png">
					</div>
				</section>
				<section>
					<h3>Skip-gram</h3>
					<small>
					<ul>
						<li>The probability of a context word $w_t$ (target) given the context's center $w_c$ is:
							\begin{align} P(w_t| w_c) &= \text{softmax}(u_t) \\
							&= \frac{\exp((v'_{w_t})^Tv_{w_c})}{\sum_{\text{word w in Vocab}} \exp((v'_{w})^Tv_{w_c})}
							\end{align}
						</li>
						<li class="fragment">The negative log-likelihood to minimize (very expensive to compute): 
						\begin{align}
						-\log P(w_t| w_c) &= - (v'_{w_t})^Tv_w \\
						&+ \log\big(\sum_{\text{word w in Vocab}} \exp((v'_{w})^Tv_{w_c})\big)
						\end{align}</li>
						<li class="fragment">Negative sampling: word2vec is instead trained using a binary classification objective:
						\begin{align} 
						 loss_{neg} &= -\log P(D=1| v'_{w_t} v_{w_c}) \\
						 &- k\mathbb{E}_{w \sim P_{noise}}\log P(D=0|v'_{w}, v_{w_c})
						 \end{align}</li>
					</ul>
				</small>
				</section>
				<section>
					<h3>Linguistic regularities in the word space</h3>
					<div class="half">
						<small>
						Analogies testing dimensions of	similarity can be solved quite well	just by	doing vector subtraction in	the	embedding space syntactically and semantically
						</small>	
						<small>
					 <table>
						<thead>
							<tr>
								<th>Expression</th>
								<th>Nearest token</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>Paris - France + Italy</td>
								<td>Rome</td>
							</tr>
							<tr>
								<td>bigger - big + cold</td>
								<td>colder</td>
							</tr>
							<tr>
								<td>sushi - Japan + Germany</td>
								<td>bratwurst</td>
							</tr>
							<tr>
								<td>Windows - Microsoft + Google</td>
								<td>Android</td>
							</tr>
							<tr>
								<td>apple - apples + cars</td>
								<td>car</td>
							</tr>
						</tbody>
					</table>
				</small>
				</div>
				<div class="half">
					<img data-src="reveal.js-3.0.0/images/word_space_reg.png">
				</div>
				</section>
				<section>
					<div class="half">
					<p>Baroni et al., “Don’t count, predict! A systematic comparision of context-counting vs. context-predicting semantic vectors”, ACL 2014</p>
					<small><p>“We set out to conduct this study [on context-counting vs.context-predicting] because we were annoyed by the triumphalist overtones often surrounding predict models, despite the almost complete lack of a proper comparison to count vectors. Our secret wish was to discover that it is all hype, and count vectors are far superior to their predictive counterparts. [...] Instead we found that the predict models are so good that, while the triumphalist overtones still sound excessive, there are very good reasons to switch to the new architecture.”</p></small>
				</div>
				<div class="half">
					<img height="700" data-src="reveal.js-3.0.0/images/baroni_paper.png">
				</div>
				</section>
				<!-- <section>
					<i>“We set out to conduct this study [on context-counting vs.context-predicting] because we were annoyed by the triumphalist overtones often surrounding predict models, despite the almost complete lack of a proper comparison to count vectors. Our secret wish was to discover that it is all hype, and count vectors are far superior to their predictive counterparts. [...] Instead we found that the predict models are so good that, while the triumphalist overtones still sound excessive, there are very good reasons to switch to the new architecture.”</i>
					<p>Baroni et al., “Don’t count, predict! A systematic comparision of context-counting vs. context-predicting semantic vectors”, ACL 2014</p>
				</section> -->
				<section>
					<h3>Language Model:
						Recurrent neural network</h3>
						<img data-src="reveal.js-3.0.0/images/sonnet.jpg">
				</section>
				<section>
					<h3>Language Model</h3>
					<ul>
						<li>A language model computes a probability for a sequence of words: $p(w_1,...,w_T)$</li>
						<li>Useful for machine translation</li>
						<ul>
							<li>Word ordering: p(the cat is small) > p(small the cat is)</li>
							<li>Word choice: p(walking home after school) > p(walking house after school)</li>
						</ul>
					</ul>
				</section>
				<section>
					<h3>Traditional language model</h3>
					<ul>
						<li>Probability is usually conditioned on window of n previous words: Markov assumption:
							\begin{align}
							&p(w_1,...,w_T) = p(w_1)\prod_{i=1}^m p(w_i|w_1,...,w_{i-1}) \\
							& \simeq p(w_1)\prod_{i=1}^m p(w_i|w_{i-(n-1)},...,w_{i-1})
							\end{align} </li>
						<li class="fragment">Estimate probability by counting:
							$p(w_2|w_1) = \frac{\text{count}(w_1, w_2)}{\text{count}(w_1)}$
							$$p(w_3| w_2, w_1) = \frac{\text{count}(w_1, w_2. w_3)}{\text{count}(w_1, w_2)}$$</li>
					</ul>
				</section>
				<section>
					<h3>N-gram: lack of Generalization</h3>
					<ul>
						<li class="fragment">Consider three trigrams: "chases a cat", "chases a dog" and "chases a rabbit" were observed in training corpus</li>
						<li class="fragment">There is a clear pattern here: "chases a" will followed by an animal.</li>
						<li class="fragment">Based on this concept, we generalize this knowledge to unseen trigrams.</li>
						<li class="fragment">N-gram language model does not capture this concept</li>
						<li class="fragment">If the trigram "chases a lama" didn't occurr more than once in the training corpus, the conditional probability given by n-gram language model will be zero.</li>
					</ul>
				</section>
				<section>
					<h3>Recurrent neural network</h3>
					<img data-src="reveal.js-3.0.0/images/fold_rnn.png">
					<small>[Credit of slide to Fei-Fei Li & Andrej Karpathy & Justin Johnson]</small>
				</section>
				<section>
					<h3>Recurrent neural network</h3>
					<ul>
						<li>Given list of word vectors: $x_1, x_2,...,x_T$</li>
						<li>$h_t = \text{sigmoid}(W^{(hh)} h_{t-1} + W^{(hx)}xt)$</li>
						<li>$y_t = \text{softmax}(W^{(yh)} h_t) $</li>
					</ul>
					<img height="400" data-src="reveal.js-3.0.0/images/rnn.png">
				</section>
				<section>
					<h4>the difficulty of training Recurrent Neural Networks: for hacker :)</h4>
					<img height="400" data-src="reveal.js-3.0.0/images/code_rnn.png">
					<small>[Credit of the slide to Fei-Fei Li & Andrej Karpathy & Justin Johnson]</small>
				</section>
				<section>
					<h4>the difficulty of training Recurrent Neural Networks: for hacker :)</h4>
					<img height="400" data-src="reveal.js-3.0.0/images/code_rnn_2.png">
					<small>[Credit of the slide to Fei-Fei Li & Andrej Karpathy & Justin Johnson]</small>
				</section>
				<section>
					<h4>the difficulty of training Recurrent Neural Networks: with maths</h4>
					<ul>
						<li >Let's take a simpler RNN
							$$h_t = Wf(h_{t-1}) + W^{(hx)}x_t$$
							$$y_t = W^{(yh)}f(h_t)$$
						</li>
						<li class="fragment">Total error is the sum of each error at time steps t: 
							$$\frac{\partial E}{\partial W} = \sum_{t=1}^T\frac{\partial E_t}{\partial W}$$</li>
						<li class="fragment">Let's apply Chain rule: $\frac{\partial E_t}{\partial W} = \sum_{k=1}^t 
							\frac{\partial E_t}{\partial y_t} \frac{\partial y_t}{\partial h_t} 
							\frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W}$ </li>
					</ul>
				</section>
				<section>
					<h4>the difficulty of training Recurrent Neural Networks: with maths</h4>
					<ul>
						<li>$\frac{\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}$</li>
						<li class="fragment">$\text{As } h_i = Wf(h_{i-1}) + W^{(hx)}x_i$, $$\frac{\partial h_i}{\partial h_{i-1}} = W^T\text{diag}(f'(h_{i-1}))$$
							</li>
						<li class="fragment">As $f'$ is bounded, $||\frac{\partial h_i}{\partial h_{i-1}}|| \leq \|W\| \| \text{diag}(f'(h_{i-1})\| \leq \beta$</li>
						<li class="fragment">$\|\frac{\partial h_t}{\partial h_k}\| \leq \prod_{i=k+1}^{t} ||\frac{\partial h_i}{\partial h_{i-1}}|| \leq \beta^{(t-k)}$</li>
						<li class="fragment">This can become very high or very small => Vanishing or exploding gradient</li>
					</ul>
				</section>
				<section>
					<h3>Trick for exploding gradient: clipping trick</h3>
					<img data-src="reveal.js-3.0.0/images/clip_trick.png">
				</section>
				<section>
					<h3>Long Short Term Memory network</h3>
					<img height="200" data-src="reveal.js-3.0.0/images/LSTM3-SimpleRNN.png">
					<img class="fragment" height="200" data-src="reveal.js-3.0.0/images/LSTM3-chain.png">
				</section>
				<section>
					<h3>Long Short Term Memory network</h3>
					<div class="half">
						<small>
					<ul>
						<li>LSTM defines an extra cell mempry $c_t$.</li>
						<li class="fragment">The cell memory $c_t$ is combined linearly at each timestamp
							$$c_t = f_{t}*c_{t-1} + i_t * \tilde{c}_t$$</li>
						<li class="fragment">The information which is propagated from $c_{t-1}$ to $c_t$ is controlled by input and forget gates $i_t$ and $f_t$.</li>
						<li class="fragment">$\tilde{c}_t$ is the memory cell candidate.
							$\tilde{c}_t = \text{tanh}(W_c[h_{t-1}, x_t] + b_c)$</li>
						<li class="fragment">The information propagated to the hidden layer is controlled by the output gate:
							$$ h_t = o_t * \text{tanh}(c_t)$$</li>
						<li class="fragment">The input gate determines the what to include from the input $x_t$.</li>
						<li class="fragment">The forget gate determines the what to forget from $c_{t−1}$.</li>
						<li class="fragment">the output gate determines what is relevant to the current state $h_t$</li>
					</ul>
				</small>
				</div>
				<div class="half">
					<img height="400" data-src="reveal.js-3.0.0/images/LSTM_block.png">
				</div>
				</section>
				<section>
					<div class="half">
						<p>Memory cell</p>
						<img class="fragment" height="200" data-src="reveal.js-3.0.0/images/LSTM3-focus-C.png">
						<p>forget gate</p>
						<img class="fragment" height="200" data-src="reveal.js-3.0.0/images/LSTM3-focus-f.png">
						
					</div>
					<div class="half">
						<p>input gate</p>
						<img class="fragment" height="200" data-src="reveal.js-3.0.0/images/LSTM3-focus-i.png">
						<p>output gate</p>
						<img class="fragment" height="200" data-src="reveal.js-3.0.0/images/LSTM3-focus-o.png">
					</div>
				</section>
				<section>
					<h3>RNN vs LSTM</h3>
					<div class ="half">
					<p>RNN</p>
					$$h_t = \text{tanh}(W \left( \begin{array}{c}x_t \\ h_{t-1} \end{array} \right)) $$
					$$ h \in \textbf{R}^n, \text{  } W [n \text{ x } 2n] $$
					</div>
					<div class="half">
					<p>LSTM</p>
					$$ \left( \begin{array}{c} i_t \\ f_t \\ o_t \\ \tilde{c}_t \end{array} \right) = 
					\left( \begin{array}{c} \text{sigmoid} \\ \text{sigmoid} \\ \text{sigmoid} \\ \text{tanh} \end{array} \right) W \left( \begin{array}{c}x_t \\ h_{t-1} \end{array} \right)$$
					$$ c_t = f_{t}*c_{t-1} + i_t * \tilde{c}_t $$
					$$ h_t = o_t * \text{tanh}(c_t) $$
					$$ h \in \textbf{R}^n, \text{  } W [4n \text{ x }2n] $$
				</div>
				</section>
				<!-- LSTMs define an extra cell memory ct, which is combined linearly at each timestamp t. The information that is propagated from ct−1 to ct is controlled by the three gates it, ft, and ot
				, which determine the what to include from the input xt, the what to forget from ct−1 and what is relevant to the current state ht
.-->
				<section>
					<h3>Language model benchmark: Google News Benchmark (1B tokens)</h3>
					<table>
						<thead>
							<tr>
								<th>model</th>
								<th>perplexity</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>5-gram + KN regularized</td>
								<td>74.4</td>
							</tr>
							<tr>
								<td>RNN [Chelba et al, 2014]</td>
								<td>51.3</td>
							</tr>
							<tr>
								<td>LSTM [Brain team, submitted to ICML 2016]</td>
								<td>30</td>
							</tr>
						</tbody>
					</table>
					<small>[Samy Bengio: college de France presentation]</small>
				</section>
				<!-- <section>
					<h3>RNN results</h3>
					<img height="300" data-src="reveal.js-3.0.0/images/rnn_baseline.png">
					<p><i>"Extensions of recurrent neural network language model"</i> by Mikolov and al 2011</p>
					<p>KN5 refers to Count-based language model with Kneser-Ney smoothing and 5-grams</p>
				</section> -->
				<section>
					<h3>Basic Information Extraction tasks</h3>
					<ul>
					<li>Part of speech tagging (POS)</li>
					<li>Named entity recognition (NER)</li>
				</ul>
				</section>
				<section>
					<h3>Information Extraction</h3>
					<div class="half">
					<p>Part Of Speech tags</p>
					<table>
						<thead>
							<tr>
								<th>tag</th>
								<th>description</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>DT</td>
								<td>Determiner</td>
							</tr>
							<tr>
								<td>JJ</td>
								<td>Adjective</td>
							</tr>
							<tr>
								<td>VB</td>
								<td>Verb, base form</td>
							</tr>
							<tr>
								<td>PRP</td>
								<td>Personal pronoun</td>
							</tr>
							<tr>
								<td>...</td>
							</tr>
						</tbody>
					</table>
				</div>
				<div class="half">
					<p>Named Entity recognition</p>
					<ul>
						<li>Organisation</li>
						<li>Date</li>
						<li>Location</li>
						<li>Person</li>
					</ul>
				</div>
				</section>
				<section>
					<h3>Classical approach</h3>
					<img data-src="reveal.js-3.0.0/images/pos_classical.png">
					<small>[Standford NLP, Chris Manning]</small>
				</section>
				<section>
					<h3>Beam Search for inference</h3>
					<ul>
						<li>Intuition: follow single path at a time but switch paths whenever some competing path look better</li>
						<li>Beam Search: keep only k most promising path (k highest or lowest scores).</li>
						<li>Beam Search example k = 3 (better = lower score): S={A} &#8594;  S={B, C, D} &#8594; S={B, C, E} &#8594; S={C, E, H}</li>
					</ul>
					<img class="fragment" height="150" data-src="reveal.js-3.0.0/images/beam1.png">
					<img class="fragment" height="150" data-src="reveal.js-3.0.0/images/beam2.png">
					<img class="fragment" height="150" data-src="reveal.js-3.0.0/images/beam3.png">
					<img class="fragment" height="150" data-src="reveal.js-3.0.0/images/beam4.png">
				</section>
				<section>
					<h3>Classical approach</h3>
					<small>
					<ul>
						<li>At each time, a classifier make a single decision conditioned on evidence from observations and previous decisions</li>
						<li class="fragment"> extract from the sentence a rich set of hand-designed features which are then fed to a standard classification algorithm, e.g. a Support Vector Machine (SVM), often with 
						a linear kernel</li>
						<li class="fragment">The choice of features is a completely empirical process, mainly based first on linguistic intuition, and then trial and error, and the feature selection is task dependent</li>
					</ul>
				</small>
					<div class="half">
						<small>
							<strong>Sentence</strong>
					<table>
						<tbody>
							<tr>
								<td>-3</td>
								<td>-2</td>
								<td>-1</td>
								<td>0</td>
								<td>+1</td>
							</tr>
							<tr>
								<td>ORG</td>
								<td>ORG</td>
								<td>O</td>
								<td>???</td>
								<td>???</td>
							</tr>
							<tr>
								<td>Xerox</td>
								<td>Corp.</td>
								<td>fell</td>
								<td>26</td>
								<td>%</td>
							</tr>
						</tbody>
					</table>
				</small>
			</div>
			<div class="half">
				<small>
					<strong>Features</strong>
					<table>
						<tbody>
							<tr>
								<td>$W_0$</td>
								<td>26</td>
							</tr>
							<tr>
								<td>$W_{+1}$</td>
								<td>%</td>
							</tr>
							<tr>
								<td>$W_{-1}$</td>
								<td>fell</td>
							</tr>
							<tr>
								<td>$C_{-1}$</td>
								<td>O</td>
							</tr>
							<tr>
								<td>$C_{-2}-C_{-1}$</td>
								<td>ORG-O</td>
							</tr>
							<tr>
								<td>hasdigit?</td>
								<td>true</td>
							</tr>
							<tr>
								<td>hasdigit?</td>
								<td>true</td>
							</tr>
							<tr>
								<td>...</td>
								<td>...</td>
							</tr>

						</tbody>
					</table>
					</small>
			</div>
				</section>
				<section>
					<h3>Natural Language Processing (almost) from Scratch</h3>
					<div class="half">
						<small>
						<ul>
							<li>Use a neural network to learn features</li>
							<li class="fragment"><strong>Embedding layer</strong>: embedding vectors are stored in $LT_W$ which is a lookup table. We embed words with the same way as in word2vec. We could extend embedding to discrete features such as word suffix.</li>
							<li class="fragment">For given input x, the network output a score $f_i(x)$ for each tag $i$.</li>
							<li class="fragment"><strong>Word-Level Log-Likelihood</strong>: the conditional tag propability is:
								$$ p(i| x) = \text{softmax}(f_i)$$</li>
						</ul>
					</small>
					</div>
					<div class="half">
					<img height="500" data-src="reveal.js-3.0.0/images/pos_nn.png">
					<small>[Collobert and al, 2011]</small>
				</div>
			</section>
				<section>
					<h3>Natural Language Processing (almost) from Scratch</h3>
					<small>
					<strong>Sentence-Level Log-Likelihood</strong>:
					<ul>
						<li>We know that there are dependencies between word tags in a sentence</li>
						<li>Tags organized in chunks and some tags cannot follow other tags</li>
						<li class="fragment">We introduce a transition score $(A_{i,j})_{(i,j)}$ for jumping from i to j tags in successive words. The transition scores are going to be trained.</li>
						<li class="fragment">The score of a sentence $[x]_1^T$ along a path of tags $[i]_1^T$ is then given by the sum of transition scores and network scores:
							$$ s([x]_1^T, [i]_1^T) = \sum_{t=1}^T (A_{i_{t-1}, i_t} + f_{i_t})$$</li>
						<li class="fragment">Then we normalize this score over all possible tag paths using softmax</li>
					</ul>
					<div class="fragment">
					<strong>Results</strong>
					<table>
						<thead>
							<tr>
								<th>Task</th>
								<th>Benchmark</th>
								<th>SENNA</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>Part of Speech (Accuracy)</td>
								<td>97.24</td>
								<td>97.29</td>
							</tr>
							<tr>
								<td>Named Entity Recognition (F1)</td>
								<td>89.31</td>
								<td>89.59</td>
							</tr>
						</tbody>
					</table>
					[Collobert and al, 2011]
				</small>
			</small>
				</section>
				<section>
					<h3>Sequence to Sequence Framework: machine translation and image captioning as applications</h3>
				</section>
				<section>
					<h3>Statistical Machine translation: classical approach</h3>
					<small>
					<ul>
						<li>Translation from source to target ($s\rightarrow t$)</li>
						<li>Source sentence: $s = s_1^m  = s_1..s_j..s_m$</li>
						<li>Target sentence: $t = t_1^I = t_1..t_i..t_I$</li>
						<li class="fragment">Search for the best translation:
							$$ t^* = \text{argmax}_t p(t|s)$$</li>
						<li class="fragment"> Bayes rule: 
							$$ t^* = \text{argmax}_t \frac{p(s|t) p(t)}{p(s)} = \text{argmax}_t p(s|t) p(t)$$</li>
						<li class="fragment">p(s|t): translation model trained on parallel data (bitexts)</li>
						<li class="fragment">p(t): language model trained on monolingual text: controls whether the target sentence is well-formed or not</li>
					</ul>
				</small>
				</section>
				<section>
					<h3>Statistical Machine translation: classical approach</h3>
					<div class="half">
						<small>
							<ul>
								<li>Perform word alignment in both directions</li>
								<li>Extract phrase (heuristic)</li>
								<li>Score phrases (relative frequency)</li>
								<li>Train language and reordering model</li>
								<li>Optimize weights of each model and combine all models</li>
							</ul>
							<strong class="fragment">&#8658 many steps which are optimized individually, no global training</strong> 
						</small>
					</div>
					<div class="half">
						<img data-src="reveal.js-3.0.0/images/smt.png">
						<small>[credit of the slide to Holger schwenk, college de France seminar]</small>
					</div>
				</section>
				<section>
					<h3>Sequence to sequence: Encoder/decoder approach</h3>
					<ul>
						<li>An encoder processes the source sentence and creates an compact representation</li>
						<li>This representation is the input to the decoder which generates a sequence in the target sentence</li>
					</ul>
					<img height="300" data-src="reveal.js-3.0.0/images/encoder_decoder.png">
				</section>

				<section>
					<img height="150" data-src="reveal.js-3.0.0/images/seq2seq_paper.png">
					<small>
					<ul>
						Some details:
						<li>Encoder/decoder: huge LSTM: 4 layers, 384M parameters and 8,000 dimensional state each.</li>
						<li>Encoder: no output layer, no loss function.</li>
						<li>SMT basline: 33.30 BLEU.</li>
						<li>Beam search for inference: 12 size.</li>
						<li>Neural Machine Tranlation system (NMT): 30.59 BLEU and 34.91 ensemble of 5.</li>
						<li>$\text{BLEU} = \text{BP} \exp{\sum_{i=1}^N \frac{\log(p_n)}{N}}$ where $p_n$ is the n-gram precision and BP is brevity penalty</li>
					</ul>
					</small>
					<img height="300" data-src="reveal.js-3.0.0/images/seq2seq_lstm.png">
					
				</section>
				<section>
					<img height="150" data-src="reveal.js-3.0.0/images/seq2seq_paper.png">
					<p>A figure of a 2-dimensional PCA projection of the LSTM hidden states that are obtained after processing the phrases</p>
					<img height="300" data-src="reveal.js-3.0.0/images/seq2seq_representation.png">

				</section>
				<section>
					<h3>The Trouble with Simple Encoder-Decoder Architectures</h3>
					<ul>
						<li>The encoder compresses the input sequence as a fixed-size vector from which the decoder needs to generate a full translation.</li>
						<li>Encoder-Decoder does not perform well with long sentences.</li>
					</ul>
					<img height="300" data-src="reveal.js-3.0.0/images/BLEU_vs_length.png">
				</section>
				<section>
					<h3>A solution: Attention mechanism</h3>
					<ul>
						<li>Let's imagine that you are translating the source sentence $(x_1,x_2...,x_n)$</li>
						<li class="fragment">You have already written the first $i-1$ target words $(y_1,y_2,..y_{i-1})$</li>
						<li class="fragment">In addition to the already generated target words, A translator looks at each source word $x_j$ and decide whether the source word $x_j$ is relevant for the next target word.</li>
						<li class="fragment">This is called attention mechanism.</li>
					</ul>
				</section>
				<section>
					<img height="100" width="400" data-src="reveal.js-3.0.0/images/attention_paper.png">
					<small>
					<ul>
						<li>The attention mechanism is implemented as a neural network with a single hidden layer and a single scalar output $e_j$. This is applied to every source word $x_j$</li>
						<li class="fragment">The attention mechanism takes as an input the previous target hidden state $z_i$ and the current source hidden state $h_i$</li>
						<li class="fragment">The relevance score of every source word is $\alpha_j = \frac{\exp(e_j)}{\sum_{j'}\exp(e_{j'})}$</li>
						<li class="fragment">The sentence representation vector is: $c_i = \sum_{j}\alpha_jh_j$. which summarizes the information about the whole sentence with different emphasis on different words.</li>
					</ul>
					</small>
					<img height="300" data-src="reveal.js-3.0.0/images/attention_net.png">
				</section>
				<section>
					<h3>Attention mechanism results</h3>
					<img height="250" data-src="reveal.js-3.0.0/images/attention_perf.png">
					<div class="fragment">
					<img height="300" height="300" data-src="reveal.js-3.0.0/images/soft_alignment.png">
					<small>Edge thicknesses represent the attention weights found by the attention model.</small>
				</div>
				</section>
				<section>
					<h3>Image captioning</h3>
					<img data-src="reveal.js-3.0.0/images/image_caption.png">
				</section>
				<section>
					<h3>Image caption with attention mecahnism</h3>
					<img data-src="reveal.js-3.0.0/images/attention_image_captioning.png">
				</section>
				<section>
					<h3>Image caption with attention mechanism</h3>
					<img height="300" data-src="reveal.js-3.0.0/images/attention_TP.png">
					<div class="fragment">
						<img height="200" data-src="reveal.js-3.0.0/images/attention_FP.png">
						<small>Examples of mistakes where we can use attention to gain intuition into what the model saw</small>
					</div>
				</section>
				<section>
					<h3>Recurrent neural network: different architectures</h3>
					<img  data-src="reveal.js-3.0.0/images/rnn_flexibility.png">
					<small>
						<ul>
							<li>One to one: Vanilla Neural Networks</li>
							<li>One to many: e.g. Image Captioning</li>
							<li>many to one: e.g. Sentiment Classification</li>
							<li>many to many: e.g. Machine Translation</li>
							<li>many to many: e.g. Video classification on frame level</li>
						</ul>
					</small>
				</section>
				<section>
					<h3>CONCLUSION</h3>
					<ul>
						<li>Deep learning provides a flexible «almost» universal learnable framework for representing world, visual and linguistic information.</li>
						<li>It is very hard to train and a lot of hyperparameters to tuned.</li>
						<li>It is as much about theory as about engineering.</li>
					</ul>
				</section>
			</div>
		</div>

		<script src="reveal.js-3.0.0/lib/js/head.min.js"></script>
		<script src="reveal.js-3.0.0/js/reveal.js"></script>



		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				math: {
					mathjax: 'reveal.js-3.0.0/MathJax-master/MathJax.js',
        			config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
    			},

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'reveal.js-3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'reveal.js-3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js-3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js-3.0.0/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'reveal.js-3.0.0/plugin/zoom-js/zoom.js', async: true },
					{ src: 'reveal.js-3.0.0/plugin/notes/notes.js', async: true },
					{ src: 'reveal.js-3.0.0/plugin/math/math.js', async: true },
					// { src: 'plugin/mathjax/MathJax.js', async: true}
				]
			});

		</script>

	</body>
</html>
