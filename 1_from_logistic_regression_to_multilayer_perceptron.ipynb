{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (10000, 28, 28), (10000,))\n",
      "('Validation set', (5000, 28, 28), (5000,))\n",
      "('Test set', (5000, 28, 28), (5000,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data reformating\n",
    "Reformat data into shape adapted to logistic regression and multiplayer percentron model\n",
    "- data as flat matrix, each images is representes of a row of 28*28 variables.\n",
    "- labels as one-hot encodings: map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (10000, 784), (10000, 10))\n",
      "('Validation set', (5000, 784), (5000, 10))\n",
      "('Test set', (5000, 784), (5000, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Softmax classifier\n",
    "Softmax is a simple classifier to assign probabilities to an object being one of several different things.\n",
    "A softmax classifier has two steps:\n",
    "1. We perform a linear transformation on the image vector $x$ of flattened pixels:\n",
    "$$ x_i \\rightarrow W*x_i + b$$\n",
    "<img src=\"images/linear_model.png\" height=\"300\">\n",
    "2. We convert that linear score into probabilities by appyling softmax function:\n",
    "$$f(x)_i = \\frac{\\exp(x_i)}{\\sum_{k=1}^N\\exp(x_k)}$$\n",
    "For training, we optimize the loss function defined by the cross-entropy.\n",
    "$$\\text{loss} = -\\sum_{i=1}^Ny\\log(\\hat{y})$$\n",
    "where $y$ is the true distribution (one-hot encoding labels) and $\\hat{y}$ is the predicted probability distribution.\n",
    "Cross-entropy is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow computational graph\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random valued following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 16.179237\n",
      "Training accuracy: 13.3%\n",
      "Validation accuracy: 15.9%\n",
      "Loss at step 100: 2.261353\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 70.9%\n",
      "Loss at step 200: 1.838964\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 73.4%\n",
      "Loss at step 300: 1.600083\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 400: 1.435220\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 500: 1.311547\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 600: 1.213903\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 700: 1.134147\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 800: 1.067530\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 75.4%\n",
      "Test accuracy: 83.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Optional: Tensorboard\n",
    "Tensorboard is visualization tool of TensorFlow's learning tasks.\n",
    "You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.\n",
    "\n",
    "For further information, you could see https://www.tensorflow.org/versions/r0.7/how_tos/summaries_and_tensorboard/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf.image_summary(\"img\", tf.reshape(tf_train_dataset, [len(train_dataset), image_size, image_size, 1]))\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random valued following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    \n",
    "    #Use a name scope to organize nodes in the graph visualizer\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Add summary ops to collect data\n",
    "    tf.histogram_summary('weights', weights)\n",
    "    tf.histogram_summary('biases', biases)\n",
    "    tf.histogram_summary('y', train_prediction)\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    tf.scalar_summary('cross entropy', loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"validation\"):\n",
    "        valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "        \n",
    "    merged = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 18.749634\n",
      "Training accuracy: 6.8%\n",
      "Validation accuracy: 8.9%\n",
      "Loss at step 100: 2.248372\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 70.1%\n",
      "Loss at step 200: 1.806984\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 73.5%\n",
      "Loss at step 300: 1.572495\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 400: 1.414999\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 75.2%\n",
      "Loss at step 500: 1.299050\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 75.6%\n",
      "Loss at step 600: 1.208796\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 75.9%\n",
      "Loss at step 700: 1.135731\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 76.1%\n",
      "Loss at step 800: 1.074747\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 76.2%\n",
      "Test accuracy: 82.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    \n",
    "    writer = tf.train.SummaryWriter(\"softmax_logs\", session.graph.as_graph_def(add_shapes=True))\n",
    "    \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions, summary_str = session.run([optimizer, loss, train_prediction, merged])\n",
    "        writer.add_summary(summary_str, step)\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "    w = weights.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get acess to Tensorboard: we type `tensorboard --logdir=softmax_logs` and we go to http://0.0.0.0:6006/ in the brower. For our softmax model, we obtain the following figures:\n",
    "<img src=\"images/softmax_graph.png\" alt=\"softmax computational graph\", height=\"400\">\n",
    "<img src=\"images/softmax_loss.png\", alt=\"loss\", height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
